# Lab 4

## 知识点

这些是与本实验有关的原理课的知识点：

* 进程状态模型
* 内核线程的控制

此外，本实验还涉及如下知识点：

* `proc`结构体及其`context`结构体的理解

遗憾的是，如下知识点在原理课中很重要，但本次实验没有很好的对应：

* 用户进程
* 挂起的进程

## 练习1

这个练习提供的注释已经写得非常详细，比实际要写的代码还要多，根据它写出C代码即可。

要实现的就是`alloc_proc`，分配一块小内存作为进程控制块，然后对其初始化。初始化操作主要是将其他字段（使用`memset`）清零，然后对特定字段，如`pid`、`cr3`等进行初始化。由于本实验是内核线程，`cr3`可以初始化为当前`cr3`（利用`rcr3`）。

与参考答案对比，我的实现使用`memset`，而参考答案对每一个字段逐一初始化。参考答案的语义更清晰，而我的性能可能会稍好一些。另外，我没有初始化`pid`为-1，在某些情况下可能出问题，应修复。此外，参考答案直接使用`boot_cr3`而不需要利用`rcr3`。

**对于`trapframe`结构体和`context`结构体作用的理解**

首先，对于每一个任务（进程或者线程）而言，保存了两组寄存器上下文信息。

一个是该任务自身控制流的上下文，一般是在该任务通过某种方式（比如中断或者系统调用）主动或被动进入内核时保存的，存于`trapframe`结构体中，存于该任务的内核栈中，此任务的控制块存放一个指向它的指针。若任务是内核线程，任务的栈与其内核栈是一个，但这不是问题。

另外一个是该任务进入内核后，对应的内核控制流的上下文，存于`context`结构体中，嵌在任务的控制块中。

值得注意的是，为统一起见，内核线程也包含有这两个部分。

这两个结构体的使用的例子请见练习3，是关于进程切换的。

## 练习2

这个练习提供的注释已经写得非常详细，比实际要写的代码还要多，根据它写出C代码即可，主要实现`do_fork`。

对于`pid`的分配，内核在`do_fork`中通过调用`get_pid`分配一个新的`pid`然后赋值给新分配的进程。下面分析`get_pid`的算法。

算法主要使用两个变量：`next_safe`以及`last_pid`。注意到它们被声明为`static`，效果类似于全局变量。首先，算法初始化`next_safe`以及`last_pid`均为`MAX_PID`，这是合法`pid`的上界加一。

每次分配，算法首先将`last_pid`增加1（特别地，若达到或超过`MAX_PID`则回到1，并认为此时`last_pid`超过了`next_safe`），此时若是仍然小于当前`next_safe`，则立即分配当前`last_pid`，本次算法结束。否则，此时说明`last_pid`已到达甚至超过`next_safe`，需要重新计算`next_safe`了。

为了接下来描述方便，假设所有未被分配的`pid`组成一段一段区间，叫做空闲`pid`区间。

计算`next_safe`的主要过程是一个循环，循环内有两个操作“同时”进行：

1. 寻找`last_pid`的下一个合法值，即大于`last_pid`的最近的空闲区间的下界，存于`last_pid`。注意，若`last_pid`被更新，`next_safe`需要重新开始计算。
2. 寻找当前`last_pid`所在空闲`pid`区间的上界加一，即已被分配的`pid`中大于`last_pid`的最小的那一个，存于`next_safe`。

总之，`last_pid`维护了最新分配出去的`pid`，而`next_safe`维护了`last_pid`所在空闲`pid`区间的上界加一。

这样就保证了每一次`get_pid`分配出来的`pid`没有其他进程正在使用，而且分配效率很高。从这里也看出，需要先分配`pid`，然后再将进程插入链表，否则算法会混乱。

与参考答案对比，由于在分配`pid`、将进程插入链表和散列表时涉及到对共享变量的操作，参考答案用关中断的方法把共享变量保护了起来，而我没有这么做，应修复。另外，对于多核处理器，应该使用其他手段，因为关中断不能确保其他处理器的互斥访问了。

## 练习3

假设系统初始化已经完成，任务都已经就绪。当一个时钟中断来临，当前任务的状态被保存到`trapframe`结构体，中断服务程序中可能会调用`schedule`选择下一个占用处理器的任务，然后在`schedule`中调用`proc_run`来切换到下一个任务。本实验没有在时钟中断服务程序中进行调度，而是在`idle`内核线程中循环调用`schedule`，原理是一致的。`proc_run`首先设置当前任务指针为下一个任务，然后加载相应的内核栈、页表，最后调用`switch_to`。可以看出，实际上切换上下文的是`switch_to`。`switch_to`完成了保存当前所有寄存器，并加载下一个任务的寄存器的动作，最后跳转到下一个任务的断点。由于这是一个返回值为`void`的函数，所以不保存和加载`eax`也没有问题。事实上，根据调用约定，只需保存和加载应该由被调用者保存的寄存器。

这样的效果就是，对于同一个任务而言，从被调度走到被调度回来，它感觉到的只是发生了中断或系统调用，中间调用了`schedule`以及`switch_to`，然后奇迹般地一层层返回了（由于我们破坏了`eax`，返回值是不确定的），最后从中断返回到断点，就像发生了一个普通的中断。它对其余任务的存在毫不知情。

于是，创建一个新的任务也很简单，只需要在它的内核栈上构造出它刚刚被中断的场景，即`trapframe`结构体，以及在任务控制块中构造出它的内核控制流的上下文，也就是`context`结构体，在下一次`proc_run`函数选中这个任务的时候，这个任务就开始执行了。`trapframe`结构体中`eip`只需设置为任务的入口点，这样，从中断返回之后便会开始执行任务；而`context`结构体中`eip`的设置需要稍作考虑，应该设置为一段能够实现从中断返回任务控制流的代码的地址，这里设置为`forkret`的入口。

对于本次实验而言，内核创建了两个内核线程：`idle`以及`init`。

`idle`是一个循环，不断释放自己的时间片，内核在没有其他进程可以运行的情况下，会选择运行此进程。

本实验的`init`进程只是打印字符串然后退出，之后的`init`进程还会进行更多的动作。

`local_intr_save(intr_flag)`是关闭中断，并将原来的中断使能状态保存到变量`intr_flag`中。

`local_intr_restore(intr_flag)`则是根据变量`intr_flag`恢复中断使能。

这两条语句中间的部分不会发生中断，若是单处理器系统，就构成了临界区。因此，这两条语句保护了切换任务上下文的过程不被中断破坏。上面提到，对于多核处理器，应该使用其他手段，因为关中断不能确保其他处理器的互斥访问了。

